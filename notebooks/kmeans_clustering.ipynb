{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Means Clustering: Global Superstore (Week 2)\n",
        "\n",
        "This notebook performs K-Means clustering on the cleaned sales dataset. It includes:\n",
        "- Scaling features\n",
        "- Initial K-Means (k=3)\n",
        "- Elbow and Silhouette methods to select k\n",
        "- PCA-based 2D visualization\n",
        "- Basic cluster interpretation and exports"
      ],
      "id": "b842588b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports and setup\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Plot style\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "\n",
        "DATA_DIR = '/workspace/data/raw'\n",
        "POSSIBLE_DIRS = [\n",
        "    os.path.join(DATA_DIR, 'GLOBAL SUPER STORE _SALES DATA ANALYSIS'),\n",
        "    DATA_DIR\n",
        "]\n",
        "OUTPUT_DIR = '/workspace/outputs'\n",
        "PROCESSED_DIR = '/workspace/data/processed'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "print('Environment ready.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5f842672"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Discover dataset files (CSV or Excel)\n",
        "candidate_files = []\n",
        "for d in POSSIBLE_DIRS:\n",
        "    if os.path.isdir(d):\n",
        "        candidate_files.extend(glob.glob(os.path.join(d, '*.csv')))\n",
        "        candidate_files.extend(glob.glob(os.path.join(d, '*.xlsx')))\n",
        "        candidate_files.extend(glob.glob(os.path.join(d, '*.xls')))\n",
        "\n",
        "print('Found files:', candidate_files)\n",
        "\n",
        "# Heuristic: prefer Excel if present with 'Global' in name, else first file\n",
        "selected_path = None\n",
        "for f in candidate_files:\n",
        "    if 'Global' in os.path.basename(f) and f.lower().endswith(('.xlsx', '.xls')):\n",
        "        selected_path = f\n",
        "        break\n",
        "if selected_path is None and candidate_files:\n",
        "    selected_path = candidate_files[0]\n",
        "\n",
        "print('Selected file:', selected_path)\n",
        "assert selected_path is not None, 'No data file found. Please place a CSV/XLSX in data/raw.'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5eb35b92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load dataset (supports CSV and Excel with sheet detection)\n",
        "file_ext = os.path.splitext(selected_path)[1].lower()\n",
        "if file_ext == '.csv':\n",
        "    df_raw = pd.read_csv(selected_path)\n",
        "else:\n",
        "    # Try common sheet names or first sheet\n",
        "    xl = pd.ExcelFile(selected_path)\n",
        "    sheet_name = None\n",
        "    for name in xl.sheet_names:\n",
        "        if str(name).strip().lower() in {'orders', 'order', 'sales', 'data'}:\n",
        "            sheet_name = name\n",
        "            break\n",
        "    df_raw = pd.read_excel(selected_path, sheet_name=sheet_name)\n",
        "\n",
        "print(df_raw.shape)\n",
        "df_raw.head(3)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9b2466e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Basic cleaning: keep numeric features, drop id/date-like columns\n",
        "# You can customize columns as needed for the dataset\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Drop obviously non-feature numeric columns if present (heuristic)\n",
        "cols_to_exclude = set()\n",
        "for col in numeric_cols:\n",
        "    if any(key in col.lower() for key in ['id']):\n",
        "        cols_to_exclude.add(col)\n",
        "\n",
        "feature_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n",
        "assert len(feature_cols) > 0, 'No numeric feature columns found.'\n",
        "\n",
        "print('Feature columns:', feature_cols)\n",
        "X = df_raw[feature_cols].copy()\n",
        "\n",
        "# Impute missing values and scale\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "print('Scaled shape:', X_scaled.shape)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "df2da789"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 5: Initial K-Means with k=3\n",
        "kmeans_initial = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
        "initial_labels = kmeans_initial.fit_predict(X_scaled)\n",
        "\n",
        "df = df_raw.copy()\n",
        "df['Cluster'] = initial_labels\n",
        "\n",
        "print('Initial inertia:', kmeans_initial.inertia_)\n",
        "df[['Cluster']].head()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5fd0af9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 6: Determine optimal clusters via Elbow and Silhouette\n",
        "inertia = []\n",
        "sil_scores = []\n",
        "K_RANGE = range(2, 11)\n",
        "for k in K_RANGE:\n",
        "    km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "    labels = km.fit_predict(X_scaled)\n",
        "    inertia.append(km.inertia_)\n",
        "    sil = silhouette_score(X_scaled, labels)\n",
        "    sil_scores.append(sil)\n",
        "\n",
        "# Plot Elbow\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(list(K_RANGE), inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'elbow.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Plot Silhouette\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(list(K_RANGE), sil_scores, marker='o', color='red')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'silhouette.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "best_k = int(K_RANGE[int(np.argmax(sil_scores))])\n",
        "print('Best k by silhouette:', best_k)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b6ebf69b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 6b: Fit KMeans with best k\n",
        "kmeans_best = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
        "best_labels = kmeans_best.fit_predict(X_scaled)\n",
        "\n",
        "df['Cluster'] = best_labels\n",
        "print('Best inertia:', kmeans_best.inertia_)\n",
        "df['Cluster'].value_counts().sort_index()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "966c83a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 7: PCA visualization (2 components)\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['Cluster'], cmap='viridis', s=18, alpha=0.8)\n",
        "plt.xlabel('PCA 1')\n",
        "plt.ylabel('PCA 2')\n",
        "plt.title('Customer Segments via PCA')\n",
        "plt.legend(*scatter.legend_elements(), title='Cluster', loc='best')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'pca_clusters.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print('Explained variance ratio:', pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1604b2b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple cluster interpretation: compute feature means by cluster\n",
        "cluster_profile = df.groupby('Cluster')[feature_cols].mean().round(2)\n",
        "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
        "\n",
        "print('Cluster counts:\\n', cluster_counts)\n",
        "cluster_profile"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "409efc71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save outputs: clustered dataset and metrics\n",
        "clustered_path = os.path.join(PROCESSED_DIR, 'clustered_data.csv')\n",
        "df.to_csv(clustered_path, index=False)\n",
        "\n",
        "metrics = {\n",
        "    'best_k': int(best_k),\n",
        "    'best_inertia': float(kmeans_best.inertia_),\n",
        "    'silhouette_scores': {int(k): float(s) for k, s in zip(K_RANGE, sil_scores)},\n",
        "    'elbow_inertia': {int(k): float(v) for k, v in zip(K_RANGE, inertia)}\n",
        "}\n",
        "with open(os.path.join(OUTPUT_DIR, 'clustering_metrics.json'), 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print('Saved:', clustered_path)\n",
        "print('Saved plots to', OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2dcc0497"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial interpretation of clusters\n",
        "\n",
        "Use the table above to infer customer segments (examples):\n",
        "- Cluster with highest average `Sales` and `Profit` → high-value customers.\n",
        "- Cluster with high `Discount` but low `Profit` → discount-sensitive, lower profitability.\n",
        "- Cluster with lower `Sales` but moderate `Quantity` → frequent small orders.\n",
        "\n",
        "Adjust based on your dataset’s feature names and domain context."
      ],
      "id": "6f29a7de"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}